# 1 Background
The internet has revolutionized the way we shop. With the advent of online shopping, we have experienced comfort and freedom of shopping almost anything, at any time, and from anywhere. But, as is the case with every technological advancement there are cons of online shopping. Disadvantages like — risk of fraud, unprecedented delays in receiving the products, lack of customized attention are to name a few.

To mitigate some of the cons, a large number of IoT (Internet of Things) sensors being used in the supply chain management these days. Supply chain is an entire system of producing and delivering a product or service, from sourcing the raw materials to the final delivery of the product or service to end users. IoT devices have added a layer of information to this management system by — tracking the location of products, both at rest and in motion; capturing real-time shipment and inventory visibility and tracking; identifying issues with goods getting lost or delayed; etc. This supply chain data generated by the IoT devices, when churned efficiently, can help the companies to make - predictions, recommendations, identify hidden patterns for better decision making in the future. However, with a lot of options available to analyze data, it often becomes difficult to decide which machine learning (ML) model to use.

# 2 Project Plan
- Use different ML models and assess their performances to perform Fraud Detection.
- Demonstrated data preprocessing steps.
- Leveraged various feature selection methods — Filter method, Wrapper method (Step-wise), Embedded (Lasso and Elastic Net) to deduce the most significant set of predictors.
- Implemented classification models to detect fraudulent transactions — Principle Component Analysis (PCA), Bagging, Random Forest, Decision Tree, K-nearest neighbor (KNN), Boosting.
- Evaluated the performance measures of these models and have preseneted a comparative study.

# 3 Dataset Description
The data set used in this project is maintained transparently with the Creative Commons 4.0 license through the Mendeley data repository. The data set consists of roughly 180k transactions from supply chains used by the company DataCo Global for over a period of three years.

### 3.1 Source
https://data.mendeley.com/datasets/8gx2fvg2k6/5

### 3.2 Description
The data set consists of 3 files:
- Transaction Data: Structured supply chain transaction data 'DataCoSupplyChainDataset.csv’.
- Log Data: Unstructured transaction log data 'tokenized access logs’.
- Column Description: Structured data 'DescriptionDataCoSupplyChain.csv’.
The ‘Transaction Data’, which has some 53 columns, has a mix of categorical and quantitative values. The primary objective is to build a fraud detection using this data set.

### 3.3 Data Visualization
-  Proportion of ‘SUSPECTED FRAUD’ is much lower than the order statuses. The data imbalance is likely to cause incorrect prediction. Therefore, up-sampling or down-sampling would be required to mitigate the data imbalance.
- Africa has the lowest number of transactions. This could be a potential market for the company.
- Maximum transaction types are ‘Debit’ and the minimmum is ‘Cash’ transactions.
- From the sales data we also find that most of the deliveries made by DataCo Global are ‘Late Delivery’. This might be a potential setback for the company’s business and turnover.

<div align="center">
  <img src="https://user-images.githubusercontent.com/82466266/234984866-bcb11d9d-601e-4ca5-b970-27f1177231cd.JPG" width=80% height=40%>
  <img src="https://user-images.githubusercontent.com/82466266/234986008-a3537f1a-8e22-4060-af2c-1293a42d2cf9.JPG" width=80% height=40%>
</div>

# 4 Data Preprocessing
The entire data set was assessed to identify missing values, NA values, irrelevant data points etc. After data preprocessing there were 38 explanantory variables and 1 response variable remaining.

- __Identify Columns Missing with Data__: Columns with more than 60% of the data missing are removed from this study. Columns with significantly large missing values will not contribute toward the prediction model. Also, imputing data for large number of missing values may lead to incorrect interpretations. Columns: ‘Order Zipcode’ and ‘Product Description’.
- __Identify Columns with Zero and Near-zero Variance__: Columns with fixed values (zero variance) or near-zero variance are identified and assessed. The zero variance columns were excluded from the data set however the near-zero variance columns were retained. Since these columns contain customer related information, they may help to identify certain trends and hence can be useful for this study.
- __Merge and Remove Redundant Columns__: Separate columns for customer first name and last name are not required. Therefore, these columns were merged. Column ‘Product Image’ was also removed since it contained the urls.
- __Create New Feature Columns__: Order and Shipping date time values were split to extract corresponding year, month, day and hours. This was done to help identify which months or days or hours are contributing toward predicting fraudulent transactions. A new column is created to label the fraud and non-fraud orders — ‘response fraud’. response fraud = (= 1, if Order Status == ’SUSPECTED FRAUD’= 0, Otherwise )
- __Training and Test set__: Data split into 70-30 ratio for training and test data set.
- __Up Sampling for Data Imbalance__: To mitigate the issue of data imbalance up sampling of the dataset was done.

# 5 Methods & Implementation
The data set contains 38 explanatory variables and 1 response variable after the preprocessing
step. The entire data was also split into 70% : 30% ratio for training and test data sets. The
following feature selection and machine learning models were applied to the transaction data set.
5.1 Feature Selection
Feature Selection (FS) is a technique to identify the most significant explanatory variables and
reduce the model size by eliminating the redundant and irrelevant variables. Feature Selection
helps to build a useful and a constructive model which is easier to interpret and trains faster.
FS helps with lowering the error due to variance (introduced by large number of predictors)
while keeping the bias error relatively low. This technique also helps to minimize the issue of
overfitting.
There are 3 different types of Feature Selection techniques:
• Filter Method: This technique comprises methods that collect the fundamental properties of the features that are measured through univariate statistics instead of using crossvalidation performance, that is the Filter method does not use the classifier. These methods are quicker and less expensive computationally. While dealing with high-dimensional
data, it is computationally cheaper to use filter methods.
• Wrapper Method: Wrappers necessitate a method to search the space of all possible
subsets of features, assessing a classifier with that feature subset, and evaluating their
quality by learning. The wrapper methods usually provide a better predictive accuracy
than filter methods. One major downside of the Wrapper technique is they are computationally heavy which is a greater problem when handling high-dim data set.
• Embedded Method: These methods cover the advantages of both filter and wrapper
methods by not only comprising interactions of features but also by retaining a reasonable
computational cost. Embedded methods combine the process of feature selection while
building the classifier, therefore, are much faster than the wrapper technique.
8
Math748 SFSU Fall’22
5.1.1 Using Filter Method
• Using filterVarImp() — The function filterVarImp() is used to rank the importance of each
predictor evaluated individually using a ‘filter’ approach. The following 9 columns were
identified as the top features. Looking closely into the variable scores, it can be observed
that ‘Customer Id’ and ‘Order Customer Id’ have the same scores, which implies that
either of the two variables could be used as an explanatory variable.
Figure 1: Important variables from Filter Method: using filterVarImp() function
• Heat Maps for Correlation: The correlation coefficients of the data set is computed and
represented in the form of a heat-map. The top 9 significant variables and the heat map
are given below. Here again we see observe that ‘Customer Id’ and ‘Order Customer Id’
have the same correlation values with respect to ‘response fraud’ variable. Hence, either
of the two should be used in the prediction model.
9
Math748 SFSU Fall’22
Figure 2: Important variables from Filter Method: using Correlation Matrix and Heat Map
5.1.2 Using Wrapper Method
Prior to using the wrapper, embedded FS methods and the subsequent classification models,
the data set was down sampled to handle data imbalance. Refer Table 1.
Table 1: Down Sampling Training Data for Fraud Detection
Minority Class (response fraud=1) Majority Class (response fraud=0)
Before 2829 123532
After 2829 2829
• Step-wise Selection (or sequential replacement) was used which is a combination of
forward and backward subset selections. This method starts with no predictors, then
sequentially adds the most influential predictors (like in forward selection). After adding
each new variable, variables that no longer provide an improvement in the model fit are
removed (like in backward selection). Step-wise identified 5 variables at a significance
level of 0.1.
10
Math748 SFSU Fall’22
Figure 3: Output of the Step-wise Feature Selection
5.1.3 Using Embedded Method
• LASSO: Using 5-folds cross-validation approach, λ1SE, λmin were computed. The λmin
was used to perform prediction on the test data set. Lasso identified 2 columns as the
most significant for fraud detection — ‘Type’ and ‘Order Country’.
• Elastic Net: Elastic Net was used in similar fashion to that of Lasso. The best α = 0.73
whereas for Lasso α = 1. Elastic Net identified only 1 column as significant — ‘Type’.
5.2 Classification Models for Fraud Detection
5.2.1 Decision Tree
Decision tree builds classification or regression models in the form of a tree structure. It breaks
down a data set into smaller and smaller subsets while at the same time an associated decision
tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A
decision node has two or more branches. Leaf node represents a classification or decision. The
topmost decision node in a tree which corresponds to the best predictor is called root node.
Decision trees can handle both categorical and numerical data. For this study, classification
decision tree is used to categorize the fraud and non-fraud transactions. the final tree structure
shows only one significant variable — ‘Type’. Test Error, Accuracy and F1 scores are calculated
for the decision tree model.
11
Math748 SFSU Fall’22
Figure 4: Classification Decision Tree
5.2.2 Bagging and Random Forest
In Bagging (Bootstrap + Aggregating) method we generate B bootstrap samples using all the
predictors and generate B# of tree structures. These trees are then aggregated to produce a
single resultant tree.
In contrast to Bagging, Random Forest uses only a subset of the predictors when creating
the splits in the tree. The subset count is given and the most common value is √p where p is the
total number of features in the original data. This selection of √p predictors is done randomly
for each split. The resultant tree formed by aggregating all the sub trees is usually found to
perform better than bagging.
Important set of variables deduced using Bagging and random Forest models are highlighted
below.
Figure 5: Important Variables from Bagging
12
Math748 SFSU Fall’22
Figure 6: Important Variables from Random Forest
5.2.3 Boosting
Boosting is a type of ensemble method where several weak learners are created at each step.
These weak learner models keep adding to produce the next model. Continuing this process
leads to a strong additive model.
5.2.4 k-Nearest Neighbors- KNN
KNN algorithm is a non-parametric, supervised learning classifier, which uses proximity to
make classifications or predictions about the grouping of an individual data point. While it can
be used for either regression or classification problems, it is typically used as a classification
algorithm, working off the assumption that similar points can be found near one another. KNN
model is implemented to predict the ‘response fraud’ for the test data set and the performance
measures are captured.
5.2.5 Principal Component Analysis - PCA
PCA is a popular technique for analyzing large data sets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the
maximum amount of information, and enabling the visualization of multidimensional data.
Formally, PCA is a statistical technique for reducing the dimensionality of a data set. This is
accomplished by linearly transforming the data into a new coordinate system where (most of)
the variation in the data can be described with fewer dimensions than the initial data.
The number of principal components to be used can be determined using a scree plot. For
this study, I have used 6 PCs which contributes for approximately 52% of the total variance.
Note that reduction in the number of variables of a data set naturally comes at the expense of
accuracy.

# 6 Code (in R)
R Code: https://github.com/ShilpikaB/Assess-Machine-Learning-Models-for-Fraud-Detection/blob/main/Math748_CapstoneProject.R


# 7 Results & Discussion
All analyses and implementations were done using the R programming language with base packages and the subsequent analysis-specific packages. 
- Variable ‘Type’ was the most significant variable indenitifed using the feature selection models. 
- Among the 8 classification models used to detect fraudulent transactions, Boosting with an interaction depth = 4 performs the best.Performance measures — lowest Test Classification Error, highest Accuracy and highest F1 score.
<div align="center">
  <img src="https://user-images.githubusercontent.com/82466266/235002599-a7ec5b3f-e1cc-455b-b103-1807c8f45f9b.JPG" width=50% height=40%>
</div>

# 8 References
- https://data.mendeley.com/datasets/8gx2fvg2k6/5
